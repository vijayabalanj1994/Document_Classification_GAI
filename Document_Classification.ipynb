{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21062004-aff8-448a-841a-44aed1093725",
   "metadata": {},
   "source": [
    "## Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ade294e1-72a6-4fec-a5d3-7adaa61254a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppressing warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d5bb31a-f096-4116-9ec5-faf2dc561597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#from tqdm import tqdm\n",
    "from itertools import accumulate\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer \n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objs as go\n",
    "from IPython.display import Markdown as md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88e8b6e-c580-4467-8380-e977e748ad4a",
   "metadata": {},
   "source": [
    "#### Checking id CUDA is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aee057d1-1181-4717-bbc8-fe62f4d7b0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847f3ca8-9103-4bb6-86bd-88800b0aa8ce",
   "metadata": {},
   "source": [
    "#### Defining helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7bf0184-6698-4fc2-a313-96ba67cc034f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(COST,ACC):\n",
    "    fig, ax1 = plt.subplots()\n",
    "    color = 'tab:red'\n",
    "    ax1.plot(COST, color=color)\n",
    "    ax1.set_xlabel('epoch', color=color)\n",
    "    ax1.set_ylabel('total loss', color=color)\n",
    "    ax1.tick_params(axis='y', color=color)\n",
    "\n",
    "    ax2 = ax1.twix()\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('accuracy', color=color)\n",
    "    ax2.plot(ACC, color=color)\n",
    "    ax2.tick_params(axis='y', color=color)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d9d6ae-420e-4f24-b01c-3e9e9cd5733f",
   "metadata": {},
   "source": [
    "## The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8fcd50-d147-4b1c-9b0d-ad0367642481",
   "metadata": {},
   "source": [
    "#### Printing an example document from the AG NEWS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b8021ec-0733-4266-a5e4-1ffadce4a950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: Business\n",
      "Text: Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n"
     ]
    }
   ],
   "source": [
    "# class labels\n",
    "ag_news_label = {1: \"World\", 2: \"Sports\", 3: \"Business\", 4: \"Sci/Tec\"}\n",
    "\n",
    "# dataset iterable object from \"torchtext\" library\n",
    "train_iter = iter(AG_NEWS(split='train'))\n",
    "\n",
    "# the 1st example document\n",
    "y, text = next(train_iter)\n",
    "print(f\"Class: {ag_news_label[y]}\\nText: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed72711b-c5c8-4372-872c-a51dc3aa8ceb",
   "metadata": {},
   "source": [
    "#### Tokenizing and building the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cedbb985-8fda-418c-aff7-eb13502eaac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2120, 12544, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# dataset iterable object from \"torchtext\" library\n",
    "train_iter = iter(AG_NEWS(split='train'))\n",
    "\n",
    "# The \"basic_english\" tokenizer from \"torchtext\" library\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# a function to get tokenized text for one document at a time\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# building the vocabulary using 'bulid_vocab_from_iterator' function from \"torchtext\" library\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"]) # This index will be returned when OOV token is queried\n",
    "\n",
    "# printing some example token indices\n",
    "print(vocab([\"age\",\"hello\",\"<unk>\",\"vijayabalan\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d6682b-c074-477a-a734-749920086d44",
   "metadata": {},
   "source": [
    "#### Spliting dataset into train, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00f84e25-e351-4a3a-856f-9fbdcd813f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.of samples in each:-\n",
      "\n",
      "train: 114000\n",
      "validation: 6000\n",
      "test: 7600\n"
     ]
    }
   ],
   "source": [
    "# spliting dataset into train and test iterators.\n",
    "train_iter, test_iter = AG_NEWS()\n",
    "\n",
    "# converting the iterators into map-style datasets using \"to_map_style_dataset\" function from \"torchtext\" library\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "# 95:5 split of train_dataset for training and validation using \"random_split\" function from \"pytorch\" library\n",
    "num_train = int(len(train_dataset)*0.95)\n",
    "split_train_dataset, split_valid_dataset = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "# printing no.of samples in each\n",
    "print(f\"No.of samples in each:-\\n\\ntrain: {num_train}\\nvalidation: {len(train_dataset) - num_train}\\ntest: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518f6c96-150e-4b56-a273-7d298f7c0732",
   "metadata": {},
   "source": [
    "#### Pre-processing pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49489cb6-4b56-4285-a9e0-3e8ce37e2ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipline to convert raw text into token indices using the \"tokenizer\" and \"vocab\" functions defined about\n",
    "def text_pipeline(x):\n",
    "    return vocab(tokenizer(x))\n",
    "\n",
    "# pipline to convert label values to start from \"0\" insted of '1'\n",
    "def label_pipeline(x):\n",
    "    return int(x) -1\n",
    "\n",
    "# a function to convert the pre-processed data returned from \"text_pipeline\" and \"label_pipeline\" into tensors for each \"batch\" from the \"dataloader\"\n",
    "def collate_batch(batch):\n",
    "    \n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    \n",
    "    for label, text in batch:\n",
    "        label_list.append(label_pipeline(label))\n",
    "        processed_text = torch.tensor(text_pipeline(text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = torch.cat(text_list)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e5889d-f81f-4182-b446-542063fb8fb5",
   "metadata": {},
   "source": [
    "#### Creating dataloaders for ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7198b030-55e4-4100-bb1d-177866782a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataloaders using \"DataLoader\" function from \"pytorch\" library\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# train dataloader\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch    \n",
    ")\n",
    "\n",
    "# validation dataloader\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch    \n",
    ")\n",
    "\n",
    "# test dataloader\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f4db78-386a-4d32-baf8-b2176e91234d",
   "metadata": {},
   "source": [
    "## Neural Network (NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088dcecb-1b3a-47a0-84df-e7387173b7a4",
   "metadata": {},
   "source": [
    "#### Defining the NN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1d02ae-f283-456b-ab58-2f088b6d97d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a feed forward 2 layer NN implemented using \"nn.EmbeddingBag\", \"nn.Linear\" functions from \"pytorch\" library\n",
    "class TextClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weights.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weights.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
