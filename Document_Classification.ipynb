{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21062004-aff8-448a-841a-44aed1093725",
   "metadata": {},
   "source": [
    "## Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ade294e1-72a6-4fec-a5d3-7adaa61254a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppressing warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d5bb31a-f096-4116-9ec5-faf2dc561597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from itertools import accumulate\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer \n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objs as go\n",
    "from IPython.display import Markdown as md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88e8b6e-c580-4467-8380-e977e748ad4a",
   "metadata": {},
   "source": [
    "#### Checking if CUDA is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aee057d1-1181-4717-bbc8-fe62f4d7b0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847f3ca8-9103-4bb6-86bd-88800b0aa8ce",
   "metadata": {},
   "source": [
    "#### Defining helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7bf0184-6698-4fc2-a313-96ba67cc034f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(COST,ACC):\n",
    "    fig, ax1 = plt.subplots()\n",
    "    color = 'tab:red'\n",
    "    ax1.plot(COST, color=color)\n",
    "    ax1.set_xlabel('epoch', color=color)\n",
    "    ax1.set_ylabel('total loss', color=color)\n",
    "    ax1.tick_params(axis='y', color=color)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('accuracy', color=color)\n",
    "    ax2.plot(ACC, color=color)\n",
    "    ax2.tick_params(axis='y', color=color)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d9d6ae-420e-4f24-b01c-3e9e9cd5733f",
   "metadata": {},
   "source": [
    "## The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8fcd50-d147-4b1c-9b0d-ad0367642481",
   "metadata": {},
   "source": [
    "#### Printing an example document from the AG NEWS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b8021ec-0733-4266-a5e4-1ffadce4a950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: Business\n",
      "Text: Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n"
     ]
    }
   ],
   "source": [
    "# class labels\n",
    "ag_news_label = {1: \"World\", 2: \"Sports\", 3: \"Business\", 4: \"Sci/Tec\"}\n",
    "num_class = 4\n",
    "\n",
    "# dataset iterable object from \"torchtext\" library\n",
    "train_iter = iter(AG_NEWS(split='train'))\n",
    "\n",
    "# the 1st example document\n",
    "y, text = next(train_iter)\n",
    "print(f\"Class: {ag_news_label[y]}\\nText: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed72711b-c5c8-4372-872c-a51dc3aa8ceb",
   "metadata": {},
   "source": [
    "#### Tokenizing and building the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cedbb985-8fda-418c-aff7-eb13502eaac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size:- 95811\n",
      "\n",
      "Tokenized text:-\n",
      " ['wall', 'st', '.', 'bears', 'claw', 'back', 'into', 'the', 'black', '(', 'reuters', ')', 'reuters', '-', 'short-sellers', ',', 'wall', 'street', \"'\", 's', 'dwindling\\\\band', 'of', 'ultra-cynics', ',', 'are', 'seeing', 'green', 'again', '.'] \n",
      "\n",
      "Token indices:-\n",
      " [431, 425, 1, 1605, 14838, 113, 66, 2, 848, 13, 27, 14, 27, 15, 50725, 3, 431, 374, 16, 9, 67507, 6, 52258, 3, 42, 4009, 783, 325, 1]\n"
     ]
    }
   ],
   "source": [
    "# dataset iterable object from \"torchtext\" library\n",
    "train_iter = iter(AG_NEWS(split='train'))\n",
    "\n",
    "# The \"basic_english\" tokenizer from \"torchtext\" library\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# a function to get tokenized text for one document at a time\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# building the vocabulary using 'bulid_vocab_from_iterator' function from \"torchtext\" library\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"]) # This index will be returned when OOV token is queried\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocab Size:- {vocab_size}\\n\")\n",
    "\n",
    "# printing the tokenized text and token indices of 1st example document\n",
    "train_iter = iter(AG_NEWS(split='train'))\n",
    "tokenized_train_iter = yield_tokens(train_iter)\n",
    "tokenized_text = next(tokenized_train_iter)\n",
    "print(\"Tokenized text:-\\n\",tokenized_text,\"\\n\")\n",
    "print(\"Token indices:-\\n\", vocab(tokenized_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d6682b-c074-477a-a734-749920086d44",
   "metadata": {},
   "source": [
    "#### Spliting dataset into train, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00f84e25-e351-4a3a-856f-9fbdcd813f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.of samples in:-\n",
      "\n",
      "train: 114000\n",
      "validation: 6000\n",
      "test: 7600\n"
     ]
    }
   ],
   "source": [
    "# spliting dataset into train and test iterators.\n",
    "train_iter, test_iter = AG_NEWS()\n",
    "\n",
    "# converting the iterators into map-style datasets using \"to_map_style_dataset\" function from \"torchtext\" library\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "# 95:5 split of train_dataset for training and validation using \"random_split\" function from \"pytorch\" library\n",
    "num_train = int(len(train_dataset)*0.95)\n",
    "split_train_dataset, split_valid_dataset = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "# printing no.of samples in each\n",
    "print(f\"No.of samples in:-\\n\\ntrain: {num_train}\\nvalidation: {len(train_dataset) - num_train}\\ntest: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518f6c96-150e-4b56-a273-7d298f7c0732",
   "metadata": {},
   "source": [
    "#### Pre-processing pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49489cb6-4b56-4285-a9e0-3e8ce37e2ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipline to convert raw text into token indices using the \"tokenizer\" and \"vocab\" functions defined about\n",
    "def text_pipeline(x):\n",
    "    return vocab(tokenizer(x))\n",
    "\n",
    "# pipline to convert label values to start from \"0\" insted of '1'\n",
    "def label_pipeline(x):\n",
    "    return int(x) -1\n",
    "\n",
    "# a function to convert the pre-processed data returned from \"text_pipeline\" and \"label_pipeline\" into tensors for each \"batch\" from the \"dataloader\"\n",
    "def collate_batch(batch):\n",
    "    \n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    \n",
    "    for label, text in batch:\n",
    "        label_list.append(label_pipeline(label))\n",
    "        processed_text = torch.tensor(text_pipeline(text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = torch.cat(text_list)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e5889d-f81f-4182-b446-542063fb8fb5",
   "metadata": {},
   "source": [
    "#### Creating dataloaders for ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7198b030-55e4-4100-bb1d-177866782a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_tensor:-\n",
      "tensor([3, 1, 0, 0, 2, 0, 3, 3, 2, 2, 0, 2, 3, 1, 3, 0, 1, 2, 1, 3, 1, 2, 3, 3,\n",
      "        2, 0, 3, 3, 3, 2, 0, 0, 2, 3, 0, 3, 1, 0, 0, 3, 1, 3, 3, 3, 0, 2, 0, 2,\n",
      "        2, 2, 2, 1, 1, 2, 2, 1, 0, 0, 0, 2, 3, 2, 0, 2], device='cuda:0')\n",
      "\n",
      "text_tensor:-\n",
      "tensor([ 205, 4073, 7565,  ...,    8, 1050,    1], device='cuda:0')\n",
      "\n",
      "offsets_tensor:-\n",
      "tensor([   0,   37,   77,  108,  154,  207,  259,  297,  386,  431,  470,  508,\n",
      "         565,  616,  672,  710,  759,  796,  834,  876,  921,  983, 1016, 1057,\n",
      "        1102, 1125, 1162, 1200, 1242, 1291, 1330, 1365, 1417, 1466, 1516, 1557,\n",
      "        1585, 1630, 1663, 1697, 1717, 1752, 1782, 1808, 1856, 1899, 1953, 1979,\n",
      "        2025, 2078, 2135, 2177, 2217, 2252, 2282, 2318, 2356, 2383, 2450, 2499,\n",
      "        2536, 2570, 2612, 2635], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# creating dataloaders using \"DataLoader\" function from \"pytorch\" library\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# train dataloader\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch    \n",
    ")\n",
    "\n",
    "# validation dataloader\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch    \n",
    ")\n",
    "\n",
    "# test dataloader\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch    \n",
    ")\n",
    "\n",
    "# printing the 1st batch of the train dataloader\n",
    "label_tensor, text_token_indices_tensor, offsets_tensor = next(iter(train_dataloader))\n",
    "print(f\"label_tensor:-\\n{label_tensor}\\n\\ntext_tensor:-\\n{text_token_indices_tensor}\\n\\noffsets_tensor:-\\n{offsets_tensor}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f4db78-386a-4d32-baf8-b2176e91234d",
   "metadata": {},
   "source": [
    "## Neural Network (NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088dcecb-1b3a-47a0-84df-e7387173b7a4",
   "metadata": {},
   "source": [
    "#### Defining the NN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc1d02ae-f283-456b-ab58-2f088b6d97d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a feed forward 2 layer NN implemented using \"nn.EmbeddingBag\", \"nn.Linear\" functions from \"pytorch\" library\n",
    "class TextClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3758565-f09c-4f7a-a23c-a05624dee55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating the model\n",
    "emsize = 64\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380701f8-3884-436e-b13a-ef0898d87612",
   "metadata": {},
   "source": [
    "#### Example of one forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4332eb5-1565-4eae-b852-237531f7f2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text token indices for the 1st document in the batch:-\n",
      "tensor([  205,  4073,  7565,  3725, 21980,   205,    26,    10,    60,    17,\n",
      "           25,    33,  2228,    68, 20688,     3,   162,  3753,   388,  4864,\n",
      "         1478,    11,   518,  2929,     6,    22,  3725,   817,   302,     6,\n",
      "         1281,   722,     8,  4976,  2344, 16629,     1], device='cuda:0')\n",
      "\n",
      "Embeddings of 1st document in the batch:-\n",
      "tensor([-1.1563e-01, -7.8998e-02, -3.6766e-02,  1.4251e-02, -4.9653e-02,\n",
      "         2.1374e-02, -5.4284e-03,  6.1491e-02, -8.7469e-02, -8.5785e-03,\n",
      "         5.5230e-02,  1.3616e-02, -1.9462e-02,  1.8341e-02, -2.8209e-03,\n",
      "         7.9919e-02,  7.1493e-02, -1.2495e-03,  9.1217e-02, -5.9095e-02,\n",
      "         4.7313e-02, -9.7084e-03,  9.5354e-03, -8.3083e-02, -1.4158e-02,\n",
      "        -2.5056e-02, -3.6265e-02,  4.6254e-02,  3.9909e-03,  1.7577e-02,\n",
      "         1.6659e-01, -6.6642e-02, -3.3816e-02, -5.4330e-02, -5.6884e-02,\n",
      "        -2.4414e-02, -7.4783e-03, -3.7485e-02, -5.7157e-03, -4.5549e-02,\n",
      "         1.5209e-02,  5.4292e-02, -2.1133e-02,  1.2761e-02, -2.1020e-02,\n",
      "         5.9465e-02, -2.4924e-02,  7.9313e-02,  3.3827e-02,  4.5640e-02,\n",
      "         8.0020e-02, -3.1109e-02, -1.0325e-01,  5.0514e-03, -9.8330e-02,\n",
      "        -3.2154e-02,  4.6566e-05, -2.9708e-03,  2.9396e-02,  3.2651e-02,\n",
      "        -1.4294e-02, -2.8897e-02, -3.1484e-02,  6.6530e-03], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "NN output of the 1st document in the batch:-\n",
      "tensor([ 0.2441, -0.2815,  0.1047, -0.0076], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "actual label:- 3\n"
     ]
    }
   ],
   "source": [
    "# getting the embedding of the text token indices for the 64 documents in the batch\n",
    "embedded_tensor = model.embedding(text_token_indices_tensor, offsets_tensor) \n",
    "# passing the embedding through the fully connected linear layer for all 64 documents in the batch\n",
    "nn_output_tensor = model.fc(embedded_tensor) \n",
    "print(f'Input text token indices for the 1st document in the batch:-\\n{text_token_indices_tensor[:offsets_tensor[1]]}\\n')\n",
    "print(f'Embeddings of 1st document in the batch:-\\n{embedded_tensor[0]}\\n')\n",
    "print(f'NN output of the 1st document in the batch:-\\n{nn_output_tensor[0]}\\n')\n",
    "print(f'actual label:- {label_tensor[0]}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1c24a3-ec7d-4b6b-8169-3de1b9c7b3db",
   "metadata": {},
   "source": [
    "#### A function to predict the class of new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80488f1b-fee0-43b1-8045-4567a4127f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New input text doc:- \"I like sports\"\n",
      "Models prediction: - Sports\n"
     ]
    }
   ],
   "source": [
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text_token_indices_tensor = torch.tensor(text_pipeline(text)).to(device)\n",
    "        offset_tensor = torch.tensor([0]).to(device)\n",
    "        nn_output_tensor = model(text_token_indices_tensor, offset_tensor)\n",
    "        return ag_news_label[nn_output_tensor.argmax(1).item()+1]\n",
    "\n",
    "print('New input text doc:- \"I like sports\"')\n",
    "print(f'Models prediction: - {predict(\"I like sports\", text_pipeline)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f8a574-e189-4f63-9ab0-26201a735d23",
   "metadata": {},
   "source": [
    "#### A function to evaluate the models performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a43ab8b2-eccf-4680-956f-58e052023588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preformance of un-trained model:- 0.25776315789473686 (accuracy)\n"
     ]
    }
   ],
   "source": [
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label_tensor, text_token_indices_tensor, offsets_tensor) in enumerate(dataloader):\n",
    "            predicted_label_tensor = model(text_token_indices_tensor, offsets_tensor)\n",
    "            total_acc += (predicted_label_tensor.argmax(1) == label_tensor).sum().item()\n",
    "            total_count += label_tensor.size(0)\n",
    "\n",
    "    return total_acc/ total_count\n",
    "\n",
    "# evaluating the un-trained model on test data\n",
    "print(f'Preformance of un-trained model:- {evaluate(test_dataloader)} (accuracy)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b962e907-15cb-4afd-9d94-a198d168cae5",
   "metadata": {},
   "source": [
    "## NN Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dae83d-9699-4164-a5ec-992ccc7d285d",
   "metadata": {},
   "source": [
    "#### The learning rate, loss criterion, optimizer and learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7f2bad0-1c19-4c25-85f9-310ca1c3f382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the learning rate, loss criterion, optimizer and learning rate scheduler using functions from the PyTorch Library \n",
    "LR = 0.1 # learning rate\n",
    "criterion = torch.nn.CrossEntropyLoss() # loss criterion \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR) # optimizer \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1) #learning rate scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42e236a-1fa1-4ac9-b202-c14ca31ce800",
   "metadata": {},
   "source": [
    "#### Training the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5813ad81-6023-48bc-b3e6-a3a840cfa865",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [01:00<00:06,  6.91s/it]"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "cum_loss_list = []\n",
    "acc_epoch =[]\n",
    "acc_old = 0\n",
    "\n",
    "for epoch in tqdm(range(1, EPOCHS+1)):\n",
    "    model.train()\n",
    "    cum_loss = 0\n",
    "    for idx, (label_tensor, text_token_indices_tensor, offsets_tensor) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label_tensor = model(text_token_indices_tensor, offsets_tensor)\n",
    "        loss = criterion(predicted_label_tensor, label_tensor)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        cum_loss += loss.item()\n",
    "    cum_loss_list.append(cum_loss)\n",
    "    acc_val = evaluate(valid_dataloader)\n",
    "    acc_epoch.append(acc_val)\n",
    "\n",
    "    if acc_val > acc_old:\n",
    "        acc_old = acc_val\n",
    "        torch.save(model.state_dict(), 'my_model.pth')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d53651-e6c6-498f-bd6c-afa9002e34db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of total loss and acccuracy on validation data over 10 epoches\n",
    "plot(cum_loss_list, acc_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee0bda2-b9d5-4ef9-946c-1ba4c6c89972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating the trained model on test data\n",
    "print(f'Preformance of un-trained model:- {evaluate(test_dataloader)} (accuracy)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
